{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "# The path to your directory\n",
    "directory = ''\n",
    "\n",
    "# Initialize a list to hold all the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Walk through every folder in the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        # Check if the file is a csv and has the correct name\n",
    "        if file.endswith('.csv') and file.startswith('ThemesandQuotevirtualreality'):\n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Read the csv file, skipping the first row\n",
    "            df = pd.read_csv(file_path, skiprows=1, names=[\"themes\", \"quotational evidence\"])\n",
    "            # Append the dataframe to the list\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now combined_df contains the combined data from all the csv files\n",
    "print(combined_df)\n",
    "combined_df.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv('combined.csv')\n",
    "\n",
    "# Create a mask where rows containing the words \"sorry\", \"apologies\" or \"apologize\" in the \"quotational evidence\" column are marked as True\n",
    "mask = df['quotational evidence'].str.contains('sorry|apologies|apologize', case=False, na=False)\n",
    "\n",
    "# Invert the mask, so that rows to be removed are marked as False\n",
    "mask = ~mask\n",
    "\n",
    "# Apply the mask to the dataframe to remove unwanted rows\n",
    "df = df[mask]\n",
    "\n",
    "# Optional: Save the modified dataframe back to csv\n",
    "df.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('combined.csv')\n",
    "\n",
    "quotations = data['quotational evidence']\n",
    "\n",
    "# tf-idf feature matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(quotations)\n",
    "\n",
    "# cosine similarity for rows\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# agglomerative Clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=0.93)\n",
    "cluster_result = cluster.fit_predict(1-cosine_sim)\n",
    "\n",
    "# create linkage matrix\n",
    "linkage_matrix = linkage(1-cosine_sim, 'average')\n",
    "\n",
    "# plot the dendrogram\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "plt.xticks(rotation=90)  # rotate x-axis labels by 90 degrees\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "data['Cluster'] = cluster_result\n",
    "\n",
    "# silhouette score for each sample\n",
    "silhouette_vals = silhouette_samples(tfidf_matrix, cluster_result)\n",
    "\n",
    "# create a dataframe of silhouette scores\n",
    "silhouette_df = pd.DataFrame()\n",
    "silhouette_df['Cluster'] = cluster_result\n",
    "silhouette_df['Silhouette Score'] = silhouette_vals\n",
    "\n",
    "# calculate average silhouette score for each cluster\n",
    "average_scores = silhouette_df.groupby('Cluster')['Silhouette Score'].mean()\n",
    "\n",
    "# map average silhouette score to each sample in the original dataframe\n",
    "data['Silhouette Score'] = data['Cluster'].map(average_scores.to_dict())\n",
    "\n",
    "data = data.sort_values('Cluster')\n",
    "data.to_csv('clusteredcombined.csv', index=False)\n",
    "print('Data has been clustered, sorted, and saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "y_lower = 10\n",
    "n_clusters = len(np.unique(cluster_result))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ith_cluster_silhouette_values = silhouette_vals[cluster_result == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "plt.title(\"The silhouette plot for the various clusters.\")\n",
    "plt.xlabel(\"The silhouette coefficient values\")\n",
    "plt.ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "average_score = np.mean(silhouette_vals)\n",
    "plt.axvline(x=average_score, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xticks(np.arange(-0.1, 1.1, 0.1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('/Users/taran/Documents/Mentorship/clusteredcombined.csv')\n",
    "\n",
    "# Analyze the 'Cluster' column\n",
    "cluster_counts = df['Cluster'].value_counts()\n",
    "\n",
    "# Sort values from most to least common\n",
    "cluster_counts_sorted = cluster_counts.sort_values(ascending=False)\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    print(cluster_counts_sorted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
